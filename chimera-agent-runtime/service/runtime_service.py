import json
import logging
import time
import traceback
from typing import Generator

# gRPC ç›¸å…³
import grpc
from rpc import runtime_pb2, runtime_pb2_grpc

# æ ¸å¿ƒç»„ä»¶
from core.stores.qdrant_store import QdrantStore
from core.stores.graph_store import NebulaStore
from core.llm.embedding import EmbeddingModel
from core.connectors.file import FileConnector
# å‡è®¾åç»­ä¼šæœ‰ FeishuConnector
# from core.connectors.feishu import FeishuConnector

# å·¥ä½œæµ (ç¨åæˆ‘ä»¬éœ€è¦è°ƒæ•´å®ƒä»¥é€‚åº”æ–°æ¶æ„)
from workflows.chat_flow import ChatWorkflow

from opentelemetry import trace

logger = logging.getLogger(__name__)
tracer = trace.get_tracer(__name__)

class ChimeraRuntimeService(runtime_pb2_grpc.RuntimeServiceServicer):
    def __init__(self, nebula_store: NebulaStore, qdrant_store: QdrantStore):
        """
        ä¾èµ–æ³¨å…¥ï¼šåœ¨ main.py ä¸­åˆå§‹åŒ–å¥½å­˜å‚¨å±‚ä¼ è¿›æ¥
        """
        self.nebula = nebula_store
        self.qdrant = qdrant_store
        # åˆå§‹åŒ– Embedding æ¨¡å‹ (å•ä¾‹)
        self.embed_model = EmbeddingModel.get_instance()
        logger.info("âœ… RuntimeService initialized with Storage Engines")

    def SyncDataSource(self, request, context):
        """
        ETL æ ¸å¿ƒå…¥å£ï¼šæ•°æ®æºåŒæ­¥
        æ”¯æŒä»ä¸åŒæº (File, Feishu) è¯»å– -> æ¸…æ´— -> å‘é‡åŒ– -> å­˜å‚¨
        """
        start_time = time.time()
        logger.info(f"ğŸ”„ [ETL] å¼€å§‹åŒæ­¥ SourceID={request.datasource_id} (Type={request.type})")

        try:
            config = json.loads(request.config_json)
            connector = None

            # 1. å·¥å‚æ¨¡å¼ï¼šé€‰æ‹©è¿æ¥å™¨
            if request.type == "file":
                # file_path é€šå¸¸æ˜¯ minio çš„è·¯å¾„æˆ–æœ¬åœ°ä¸´æ—¶è·¯å¾„
                connector = FileConnector(request.kb_id, request.datasource_id, config)
            elif request.type == "feishu":
                # connector = FeishuConnector(request.kb_id, request.datasource_id, config)
                raise NotImplementedError("é£ä¹¦è¿æ¥å™¨å¼€å‘ä¸­")
            else:
                return runtime_pb2.SyncResponse(success=False, error_msg=f"æœªçŸ¥çš„ç±»å‹: {request.type}")

            chunks_buffer = []
            total_chunks = 0

            # 2. æµå¼å¤„ç†ï¼šè¯»å– -> å‘é‡åŒ–
            # connector.load() æ˜¯ä¸€ä¸ªç”Ÿæˆå™¨ï¼Œè¿”å› DocumentChunk å¯¹è±¡
            for chunk in connector.load():
                # è®¡ç®—å‘é‡ (384ç»´)
                vector = self.embed_model.encode(chunk.content)

                # ç»„è£… Qdrant éœ€è¦çš„æ•°æ®ç»“æ„
                chunks_buffer.append({
                    "vector": vector,
                    "payload": {
                        "content": chunk.content,
                        "kb_id": request.kb_id,
                        "source_id": request.datasource_id,
                        **chunk.metadata # åˆå¹¶å…¶ä»–å…ƒæ•°æ® (å¦‚ page_num)
                    }
                })

                # æ‰¹å¤„ç†å†™å…¥ (æ¯ 50 æ¡å†™ä¸€æ¬¡ï¼Œé˜²æ­¢å†…å­˜æº¢å‡º)
                if len(chunks_buffer) >= 50:
                    self.qdrant.upsert_chunks(chunks_buffer)
                    total_chunks += len(chunks_buffer)
                    chunks_buffer = []

            # å†™å…¥å‰©ä½™çš„
            if chunks_buffer:
                self.qdrant.upsert_chunks(chunks_buffer)
                total_chunks += len(chunks_buffer)

            logger.info(f"âœ… [ETL] åŒæ­¥å®Œæˆã€‚å…±å†™å…¥ {total_chunks} ä¸ªåˆ‡ç‰‡ï¼Œè€—æ—¶ {time.time() - start_time:.2f}s")

            return runtime_pb2.SyncResponse(success=True, chunks_count=total_chunks)

        except Exception as e:
            logger.error(f"âŒ [ETL] åŒæ­¥å¤±è´¥: {str(e)}")
            logger.error(traceback.format_exc())
            return runtime_pb2.SyncResponse(success=False, error_msg=str(e))

    def RunAgent(self, request, context):
        """
        æ¨ç†æ ¸å¿ƒå…¥å£ï¼šæ‰§è¡Œ Agent
        """
        start_time = time.time() # â±ï¸ è®¡æ—¶å¼€å§‹
        # ... TraceID è·å– ...

        # åˆå§‹åŒ–ç»Ÿè®¡
        usage_stats = {
            "prompt_tokens": 0,
            "completion_tokens": 0,
            "total_tokens": 0
        }
        status = "success"
        # è·å–å½“å‰çš„ TraceID (ç”± Go ç«¯é€ä¼ æˆ–è‡ªåŠ¨ç”Ÿæˆ)
        current_span = trace.get_current_span()
        trace_id = format(current_span.get_span_context().trace_id, "032x")

        logger.info(f"ğŸ¤– [RunAgent] AppID={request.app_id} Query={request.query[:20]}...")

        try:
            # 1. è§£æé…ç½®
            app_config = json.loads(request.app_config_json)
            # æå– kb_ids, ä¾‹å¦‚: [1, 2]
            kb_ids = app_config.get("kb_ids", [])

            # 2. åˆå§‹åŒ–å·¥ä½œæµ (LangGraph)
            # æ³¨æ„ï¼šæˆ‘ä»¬å°† qdrant_store å’Œ kb_ids ä¼ å…¥å·¥ä½œæµï¼Œè¿™æ ·å®ƒæ‰èƒ½å»æ£€ç´¢æ­£ç¡®çš„æ•°æ®
            workflow = ChatWorkflow(self.nebula, self.qdrant, kb_ids)

            # æ„é€ åˆå§‹çŠ¶æ€
            initial_state = {
                "query": request.query,
                "history": request.history, # æš‚æ—¶é€ä¼ 
                "app_config": app_config
            }

            # 3. è¿è¡Œå·¥ä½œæµå¹¶æµå¼è¿”å›
            # å‡è®¾ workflow.run_stream è¿”å›çš„æ˜¯ä¸€ä¸ªç”Ÿæˆå™¨ï¼Œäº§ç”Ÿäº‹ä»¶
            for event in workflow.run_stream(initial_state):

                # A. å¤„ç†æ€è€ƒäº‹ä»¶ (thought)
                if event["type"] == "thought":
                    yield runtime_pb2.RunAgentResponse(
                        type="thought",
                        payload=event["content"],
                        meta=runtime_pb2.AgentMeta(
                            node_name=event.get("node", "Agent"),
                            trace_id=trace_id,
                            duration_ms=event.get("duration", 0)
                        )
                    )

                # B. å¤„ç†ç­”æ¡ˆç‰‡æ®µ (Delta)
                elif event["type"] == "delta":
                    yield runtime_pb2.RunAgentResponse(
                        type="delta",
                        payload=event["content"]
                    )

                # C. å¤„ç†å¼•ç”¨ (Reference)
                elif event["type"] == "reference":
                    yield runtime_pb2.RunAgentResponse(
                        type="reference",
                        payload=json.dumps(event["docs"]) # åºåˆ—åŒ–å¼•ç”¨åˆ—è¡¨
                    )

                # D. æ•è· Usage äº‹ä»¶
                elif event["type"] == "usage":
                    u = event["usage"]
                    usage_stats["prompt_tokens"] = u.get("prompt_tokens", 0)
                    usage_stats["completion_tokens"] = u.get("completion_tokens", 0)
                    usage_stats["total_tokens"] = u.get("total_tokens", 0)

        except Exception as e:
            logger.error(f"âŒ [RunAgent] æ‰§è¡Œå¼‚å¸¸: {str(e)}")
            yield runtime_pb2.RunAgentResponse(
                type="error",
                payload=f"System Error: {str(e)}"
            )

        finally:
            # ğŸ”¥ æœ€ç»ˆï¼šå‘é€ Summary
            duration = int((time.time() - start_time) * 1000)

            logger.info(f"ğŸ“Š [Summary] Duration={duration}ms Tokens={usage_stats['total_tokens']}")

            yield runtime_pb2.RunAgentResponse(
                type="summary",
                summary=runtime_pb2.RunSummary(
                    total_tokens=usage_stats["total_tokens"],
                    prompt_tokens=usage_stats["prompt_tokens"],
                    completion_tokens=usage_stats["completion_tokens"],
                    total_duration_ms=duration,
                    final_status=status
                )
            )