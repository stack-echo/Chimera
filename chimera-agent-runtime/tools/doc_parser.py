import logging
import io
from pathlib import Path

# Docling æ ¸å¿ƒç»„ä»¶
from docling.document_converter import DocumentConverter, PdfFormatOption
from docling.datamodel.base_models import InputFormat
from docling.datamodel.pipeline_options import PdfPipelineOptions
from docling.datamodel.document import DocumentStream

# ğŸ”¥ å…³é”®ï¼šHybridChunker åœ¨ docling.chunking ä¸‹
from docling.chunking import HybridChunker

class DoclingParser:
    _converter = None
    _chunker = None
    def __init__(self):
        # é¢„åŠ è½½æ¨¡å‹ï¼Œé¿å…åœ¨è¯·æ±‚ä¸­åˆå§‹åŒ–
        self.converter = DocumentConverter()

    @classmethod
    def _get_components(cls):
        """å•ä¾‹æ¨¡å¼åˆå§‹åŒ– Converter å’Œ Chunker"""
        if cls._converter is None:
            logging.info("ğŸ¢ [Init] æ­£åœ¨åˆå§‹åŒ– Docling æ¨¡å‹ (HybridChunker enabled)...")

            # 1. é…ç½®è½¬æ¢å™¨
            pipeline_options = PdfPipelineOptions()
            pipeline_options.do_ocr = False
            pipeline_options.do_table_structure = True

            cls._converter = DocumentConverter(
                format_options={
                    InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)
                }
            )

            # 2. é…ç½®åˆ‡åˆ†å™¨ (HybridChunker)
            # ä½¿ç”¨ sentence-transformers çš„ tokenizer æ¥è®¡ç®— token æ•°ï¼Œç¡®ä¿åˆ‡ç‰‡ä¸ä¼šè¶…é•¿
            cls._chunker = HybridChunker(
                tokenizer="sentence-transformers/all-MiniLM-L6-v2",
                max_tokens=500, # é€‚åˆ embedding æ¨¡å‹çš„çª—å£å¤§å°
                merge_peers=True,
            )

            logging.info("âœ… [Init] Docling ç»„ä»¶å°±ç»ª")
        return cls._converter, cls._chunker

    @staticmethod
    def parse_and_chunk(file_source, filename="temp.pdf"):
        """
        è§£æ PDF å¹¶è¿”å›å¸¦æœ‰ã€çœŸå®é¡µç ã€‘çš„è¯­ä¹‰åˆ‡ç‰‡
        :param file_source: å¯ä»¥æ˜¯ str (è·¯å¾„), Path (è·¯å¾„), æˆ– bytes (äºŒè¿›åˆ¶)
        """
        converter, chunker = DoclingParser._get_components()
        logging.info(f"ğŸ“„ [Docling] å¼€å§‹è§£æ: {filename}")

        try:
            # 1. æ™ºèƒ½æ„å»ºè¾“å…¥æº
            input_doc = None

            if isinstance(file_source, bytes):
                # Case A: ä¼ å…¥äºŒè¿›åˆ¶æµ (å†…å­˜å¤„ç†)
                logging.info(f"   âš™ï¸ Mode: Bytes Stream ({len(file_source)} bytes)")
                input_doc = DocumentStream(name=filename, stream=io.BytesIO(file_source))
            elif isinstance(file_source, (str, Path)):
                # Case B: ä¼ å…¥æ–‡ä»¶è·¯å¾„ (æ¨èï¼Œæ€§èƒ½æ›´å¥½ä¸”ç¨³å®š)
                logging.info(f"   âš™ï¸ Mode: File Path ({file_source})")
                input_doc = Path(file_source)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„è¾“å…¥ç±»å‹: {type(file_source)}")

            # 2. æ‰§è¡Œè½¬æ¢ (PDF -> DL Document)
            conv_result = converter.convert(input_doc)
            doc = conv_result.document
            logging.info(f"âœ… [Docling] è½¬æ¢å®Œæˆï¼Œå¼€å§‹ HybridChunker åˆ‡åˆ†...")

            # 3. ä½¿ç”¨ HybridChunker åˆ‡åˆ†
            chunk_iter = chunker.chunk(doc)

            final_chunks = []
            for i, chunk in enumerate(chunk_iter):
                # ğŸ”¥ æå–é¡µç  (è¿½æº¯ Provenance)
                page_num = 1
                if chunk.meta.doc_items:
                    first_item = chunk.meta.doc_items[0]
                    if hasattr(first_item, 'prov') and first_item.prov:
                        page_num = first_item.prov[0].page_no

                # åºåˆ—åŒ–ç»“æœ
                # chunk.text å·²ç»åŒ…å«äº†ä¸Šä¸‹æ–‡ï¼ˆå¦‚æ ‡é¢˜ï¼‰
                final_chunks.append({
                    "content": chunk.text,
                    "page": page_num
                })

            logging.info(f"âœ‚ï¸ [HybridChunker] ç”Ÿæˆäº† {len(final_chunks)} ä¸ªå¸¦æœ‰é¡µç çš„ç‰‡æ®µ")

            return final_chunks

        except Exception as e:
            logging.error(f"âŒ [Docling] è§£æå¤±è´¥: {e}", exc_info=True)
            return []