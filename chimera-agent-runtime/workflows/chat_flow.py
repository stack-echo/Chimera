import json
import logging
from typing import TypedDict, List, Dict, Any, Generator
from langgraph.graph import StateGraph, END

from core.llm.embedding import EmbeddingModel
from core.llm.llm import LLMClient
from core.stores.qdrant_store import QdrantStore
from core.stores.graph_store import NebulaStore

logger = logging.getLogger(__name__)

# --- çŠ¶æ€å®šä¹‰ ---
class AgentState(TypedDict):
    query: str
    chat_history: List[Dict[str, str]] # [{"role": "user", "content": "..."}]

    # ä¸Šä¸‹æ–‡
    retrieved_docs: List[Dict]
    graph_context: str

    # æœ€ç»ˆç­”æ¡ˆ
    answer: str

class ChatWorkflow:
    def __init__(self, nebula: NebulaStore, qdrant: QdrantStore, kb_ids: List[int]):
        """
        åˆå§‹åŒ–å·¥ä½œæµï¼Œæ³¨å…¥èµ„æº
        """
        self.nebula = nebula
        self.qdrant = qdrant
        self.kb_ids = kb_ids

        # åˆå§‹åŒ–æ¨¡å‹
        self.embed_model = EmbeddingModel.get_instance()
        self.llm = LLMClient()

        # æ„å»ºå›¾
        self.app = self._build_graph()

    def _build_graph(self):
        workflow = StateGraph(AgentState)

        # å®šä¹‰èŠ‚ç‚¹
        workflow.add_node("retrieve", self.node_retrieve)
        workflow.add_node("generate", self.node_generate)

        # å®šä¹‰è¾¹
        workflow.set_entry_point("retrieve")
        workflow.add_edge("retrieve", "generate")
        workflow.add_edge("generate", END)

        return workflow.compile()

    # --- èŠ‚ç‚¹é€»è¾‘ ---

    def node_retrieve(self, state: AgentState):
        """
        æ£€ç´¢èŠ‚ç‚¹ï¼šåŒæ—¶æŸ¥è¯¢ å‘é‡åº“(Qdrant) å’Œ å›¾æ•°æ®åº“(Nebula)
        """
        query = state["query"]
        logger.info(f"ğŸ” [Retrieve] æ­£åœ¨æ£€ç´¢: {query} (KB IDs: {self.kb_ids})")

        # 1. å‘é‡æ£€ç´¢ (Qdrant)
        try:
            query_vector = self.embed_model.encode(query)
            # è°ƒç”¨æˆ‘ä»¬åˆšå†™çš„ search æ–¹æ³•ï¼Œä¼ å…¥ kb_ids è¿‡æ»¤
            vector_results = self.qdrant.search(
                query_vector=query_vector,
                kb_ids=self.kb_ids,
                top_k=5
            )
        except Exception as e:
            logger.error(f"Qdrant Search Error: {e}")
            vector_results = []

        # 2. å›¾æ£€ç´¢ (Nebula) - ç®€å•ç¤ºä¾‹ï¼šæŸ¥æ‰¾åŒ…å«å…³é”®è¯çš„å®ä½“
        # (è¿™é‡Œä¸ºäº†ç¨³å¥ï¼Œå¦‚æœå›¾æ²¡å‡†å¤‡å¥½ï¼Œå…ˆ try-catch æ‰)
        graph_text = ""
        try:
            # è¿™é‡Œçš„é€»è¾‘å¯ä»¥åšå¾—å¾ˆå¤æ‚ï¼Œæ¯”å¦‚æå–å®ä½“ -> æŸ¥å­å›¾
            # è¿™é‡Œä»…ä½œå ä½ï¼Œé˜²æ­¢æŠ¥é”™
            pass
        except Exception as e:
            logger.error(f"Nebula Search Error: {e}")

        return {
            "retrieved_docs": vector_results,
            "graph_context": graph_text
        }

    def node_generate(self, state: AgentState):
        """
        ç”ŸæˆèŠ‚ç‚¹ï¼šç»„è£… Prompt ä½†ä¸ç›´æ¥è°ƒç”¨ LLMã€‚
        è¿™é‡Œæˆ‘ä»¬ä¸åšå®é™…ç”Ÿæˆï¼Œè€Œæ˜¯å‡†å¤‡å¥½ä¸Šä¸‹æ–‡ï¼Œå®é™…çš„æµå¼ç”Ÿæˆåœ¨ run_stream é‡Œè§¦å‘ã€‚
        """
        # ä»…åšçŠ¶æ€ä¼ é€’ï¼ŒLangGraph è¿è¡Œå®Œè¿™ä¸ªèŠ‚ç‚¹åï¼Œæˆ‘ä»¬ä¼šæ‹¿åˆ° state
        return {}

    # --- æ ¸å¿ƒè¿è¡Œé€»è¾‘ ---

    def run_stream(self, initial_state: dict) -> Generator[Dict[str, Any], None, None]:
        """
        æ‰§è¡Œå·¥ä½œæµï¼Œå¹¶ä»¥ç”Ÿæˆå™¨å½¢å¼è¿”å›äº‹ä»¶
        è¿™é€‚é…äº† runtime_service.py çš„è°ƒç”¨æ–¹å¼
        """

        # 1. å‘é€â€œæ€è€ƒâ€äº‹ä»¶
        yield {
            "type": "thought",
            "node": "Retrieve",
            "content": "æ­£åœ¨çŸ¥è¯†åº“ä¸­æ£€ç´¢ç›¸å…³æ–‡æ¡£...",
            "duration": 0
        }

        # 2. è¿è¡Œæ£€ç´¢èŠ‚ç‚¹ (æ‰‹åŠ¨ invoke graph çš„ä¸€éƒ¨åˆ†ï¼Œæˆ–è€…è¿è¡Œæ•´ä¸ª graph æ‹¿åˆ°ç»“æœ)
        # ä¸ºäº†ç®€å•ï¼Œæˆ‘ä»¬è¿™é‡Œç›´æ¥è¿è¡Œ LangGraphï¼Œæ‹¿åˆ°æ£€ç´¢ç»“æœ
        # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ invoke åŒæ­¥æ‰§è¡Œæ£€ç´¢ï¼Œå› ä¸ºæ£€ç´¢é€šå¸¸å¾ˆå¿«

        # æ„é€  LangGraph éœ€è¦çš„è¾“å…¥
        input_state = {
            "query": initial_state["query"],
            "chat_history": initial_state.get("history", []),
            "retrieved_docs": [],
            "graph_context": "",
            "answer": ""
        }

        # è¿è¡Œå›¾ (ç›´åˆ° retrieve å®Œæˆ)
        # è¿™é‡Œæœ‰ä¸€ä¸ªæŠ€å·§ï¼šæˆ‘ä»¬æ‰‹åŠ¨è°ƒç”¨èŠ‚ç‚¹é€»è¾‘ï¼Œä»¥ä¾¿æ›´å¥½æ§åˆ¶æµå¼è¾“å‡º
        # æˆ–è€…ï¼Œæˆ‘ä»¬å¯ä»¥è¿è¡Œ app.invoke(input_state) æ‹¿åˆ° context

        # === æ‰‹åŠ¨æ‰§è¡Œ Retrieval é˜¶æ®µ ===
        retrieve_output = self.node_retrieve(input_state)
        docs = retrieve_output["retrieved_docs"]

        # å‘é€å¼•ç”¨äº‹ä»¶
        if docs:
            formatted_docs = []
            for doc in docs:
                meta = doc.get("metadata", {})
                formatted_docs.append({
                    "file_name": meta.get("file_name", "unknown"),
                    "page": meta.get("page_number", 1),
                    "score": doc.get("score", 0),
                    "snippet": doc.get("content", "")[:100] + "..."
                })
            yield {
                "type": "reference",
                "docs": formatted_docs
            }

        # === æ‰§è¡Œ Generation é˜¶æ®µ ===
        yield {
            "type": "thought",
            "node": "Generate",
            "content": "æ­£åœ¨æ•´ç†æ£€ç´¢ç»“æœå¹¶ç”Ÿæˆå›ç­”...",
            "duration": 0
        }

        # 3. ç»„è£… Prompt
        context_str = "\n\n".join([f"[æ–‡æ¡£ç‰‡æ®µ]: {d['content']}" for d in docs])
        if not context_str:
            context_str = "æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ï¼Œè¯·æ ¹æ®å¸¸è¯†å›ç­”ã€‚"

        system_prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¼ä¸šçŸ¥è¯†åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä»¥ä¸‹å‚è€ƒèµ„æ–™å›ç­”ç”¨æˆ·é—®é¢˜ã€‚
å¦‚æœå‚è€ƒèµ„æ–™æ— æ³•å›ç­”é—®é¢˜ï¼Œè¯·è¯šå®è¯´æ˜ã€‚

ã€å‚è€ƒèµ„æ–™ã€‘ï¼š
{context_str}
"""
        # Query ç‹¬ç«‹ä¼ é€’
        # åœ¨ llm.stream_chat ä¸­ä¼šå¤„ç† messages

        # 4. è°ƒç”¨ LLM æµå¼ç”Ÿæˆ
        # è¿™é‡Œç›´æ¥è°ƒç”¨ LLMClientï¼Œç»•è¿‡ Graph çš„é™æ€è¿”å›ï¼Œå®ç° Token æµ
        try:
            for event in self.llm.stream_chat(
                    query=initial_state['query'],
                    system_prompt=system_prompt,
                    history=initial_state.get("history", [])
            ):
                # é€ä¼ å†…å®¹
                if event["type"] == "content":
                    yield {
                        "type": "delta",
                        "content": event["data"]
                    }
                # ğŸ”¥ é€ä¼  Usage
                elif event["type"] == "usage":
                    yield {
                        "type": "usage",
                        "usage": event["data"]
                    }
        except Exception as e:
            logger.error(f"LLM Stream Error: {e}")
            yield {
                "type": "delta",
                "content": f"[ç”Ÿæˆå‡ºé”™: {str(e)}]"
            }