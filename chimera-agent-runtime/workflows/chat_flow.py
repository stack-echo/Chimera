import json
import logging
import os # ğŸ”¥ æ–°å¢å¼•å…¥
import yaml # ğŸ”¥ æ–°å¢å¼•å…¥
from typing import TypedDict, List, Dict, Any, Generator
from langgraph.graph import StateGraph, END
from jinja2 import Template # ğŸ”¥ æ–°å¢å¼•å…¥

from core.llm.embedding import EmbeddingModel
from core.llm.llm import LLMClient
from core.stores.qdrant_store import QdrantStore
from core.stores.graph_store import NebulaStore
# ğŸ”¥ å¼•å…¥ QueryAnalysisAgent
from agents.chat.query_analysis import QueryAnalysisAgent

logger = logging.getLogger(__name__)

# --- çŠ¶æ€å®šä¹‰ ---
class AgentState(TypedDict):
    query: str
    chat_history: List[Dict[str, str]]

    query_entities: List[str] # ä» QueryAnalysisAgent è·å–

    retrieved_docs: List[Dict]
    graph_context: List[str]   # å›¾è°±ç»“æœ (ä¸‰å…ƒç»„å­—ç¬¦ä¸²)

    answer: str

class ChatWorkflow:
    def __init__(self, nebula: NebulaStore, qdrant: QdrantStore, kb_ids: List[int]):
        self.nebula = nebula
        self.qdrant = qdrant
        self.kb_ids = kb_ids

        self.embed_model = EmbeddingModel.get_instance()
        self.llm = LLMClient()

        # ğŸ”¥ åˆå§‹åŒ– QueryAnalysisAgent
        self.query_analyzer = QueryAnalysisAgent()

        # ğŸ”¥ åŠ è½½ç”Ÿæˆ Prompt
        self.synthesis_prompt_config = self._load_prompt("chat/synthesis.yaml")

        self.app = self._build_graph()

    # ğŸ”¥ æ–°å¢ï¼šåŠ è½½ Prompt çš„è¾…åŠ©æ–¹æ³•
    def _load_prompt(self, filename):
        base_dir = os.getcwd()
        if "chimera-agent-runtime" not in base_dir and os.path.exists("chimera-agent-runtime"):
            base_dir = os.path.join(base_dir, "chimera-agent-runtime")
        path = os.path.join(base_dir, "prompts", filename)

        if not os.path.exists(path):
            raise FileNotFoundError(f"âŒ æç¤ºè¯æ–‡ä»¶æœªæ‰¾åˆ°: {path}")
        with open(path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)

    def _build_graph(self):
        workflow = StateGraph(AgentState)

        # å®šä¹‰èŠ‚ç‚¹
        workflow.add_node("query_analysis", self.node_query_analysis) # ğŸ”¥ æ–°å¢èŠ‚ç‚¹
        workflow.add_node("retrieve", self.node_retrieve)
        workflow.add_node("generate", self.node_generate)

        # è¿çº¿
        workflow.set_entry_point("query_analysis") # ğŸ”¥ å…¥å£æ”¹ä¸º Query Analysis
        workflow.add_edge("query_analysis", "retrieve")
        workflow.add_edge("retrieve", "generate")
        workflow.add_edge("generate", END)

        return workflow.compile()

    # --- èŠ‚ç‚¹é€»è¾‘ ---

    def node_query_analysis(self, state: AgentState):
        """æ­¥éª¤ 1: åˆ†æç”¨æˆ· Queryï¼Œæå–å…³é”®å®ä½“"""
        logger.info(f"ğŸ§  [Chat-1] Query Analysis Agent æ­£åœ¨åˆ†æ: {state['query']}")
        entities = self.query_analyzer.run(state["query"])
        logger.info(f"   -> è¯†åˆ«åˆ°å®ä½“: {entities}")
        return {"query_entities": entities}

    def node_retrieve(self, state: AgentState):
        """æ­¥éª¤ 2: åŒè·¯æ£€ç´¢ (Vector + Graph)"""
        query = state["query"]
        entities = state.get("query_entities", []) # ğŸ”¥ ä»ä¸Šä¸€ä¸ªèŠ‚ç‚¹è·å–å®ä½“

        if not entities: # å¦‚æœæ²¡æœ‰æå–åˆ°å®ä½“ï¼Œå°è¯•ç”¨åŸå§‹ query ä½œä¸ºå…œåº•
            entities = [query]

        logger.info(f"ğŸ” [Chat-2] æ­£åœ¨æ£€ç´¢: {query} (KB IDs: {self.kb_ids}) with Entities: {entities}")

        # A. å‘é‡æ£€ç´¢
        vector_results = []
        try:
            query_vec = self.embed_model.encode(query)
            vector_results = self.qdrant.search(query_vec, self.kb_ids, top_k=3)
        except Exception as e:
            logger.error(f"Vector Search Error: {e}")

        # B. å›¾è°±æ£€ç´¢
        graph_triplets = []
        try:
            # è°ƒç”¨ Storeï¼Œä½¿ç”¨æå–çš„å®ä½“
            graph_triplets = self.nebula.retrieve_subgraph(entities)
            logger.info(f"ğŸ•¸ï¸ [Chat-2] çŸ¥è¯†å›¾è°±å‘½ä¸­ {len(graph_triplets)} æ¡å…³è”çŸ¥è¯†")

        except Exception as e:
            logger.error(f"Graph Search Error: {e}")

        return {
            "retrieved_docs": vector_results,
            "graph_context": graph_triplets
        }

    def node_generate(self, state: AgentState):
        # è¿™ä¸ªèŠ‚ç‚¹ç°åœ¨åªåšçŠ¶æ€ä¼ é€’ï¼Œå®é™…çš„ Prompt æ¸²æŸ“åœ¨ run_stream ç»Ÿä¸€å¤„ç†
        return {}

    # --- æ ¸å¿ƒè¿è¡Œé€»è¾‘ ---

    def run_stream(self, initial_state: dict) -> Generator[Dict[str, Any], None, None]:
        # 1. æ‰§è¡Œ LangGraph è·å–æœ€ç»ˆçŠ¶æ€
        # æˆ‘ä»¬ä½¿ç”¨ app.invoke æ¥åŒæ­¥æ‰§è¡Œï¼Œæ‹¿åˆ°æœ€ç»ˆçŠ¶æ€
        final_state = self.app.invoke(initial_state)

        # ä»æœ€ç»ˆçŠ¶æ€ä¸­è·å–æ£€ç´¢ç»“æœ
        query = final_state["query"]
        vec_docs = final_state.get("retrieved_docs", [])
        graph_triplets = final_state.get("graph_context", [])

        # 2. ç»„è£… Prompt
        doc_context_str = "\n".join([f"- {d['content']}" for d in vec_docs])
        if not doc_context_str:
            doc_context_str = "æ— ç›¸å…³æ–‡æ¡£ç‰‡æ®µã€‚"

        kg_context_str = "\n".join(graph_triplets)
        if not kg_context_str:
            kg_context_str = "æ— ç›¸å…³çŸ¥è¯†å›¾è°±ä¿¡æ¯ã€‚"

        full_context = f"""
        ã€æ–‡æ¡£ç‰‡æ®µã€‘ï¼š
        {doc_context_str}
        
        ã€çŸ¥è¯†å›¾è°±è·¯å¾„ã€‘ï¼š
        {kg_context_str}
        """
        # ğŸ”¥ ä»é…ç½®æ–‡ä»¶åŠ è½½ System å’Œ User Prompt
        sys_tmpl = self.synthesis_prompt_config.get("system", "")
        user_tmpl = self.synthesis_prompt_config.get("user", "")

        system_prompt = Template(sys_tmpl).render(full_context=full_context)
        user_prompt_content = Template(user_tmpl).render(query=query) # user_prompt åªåŒ…å« query

        # 3. è°ƒç”¨ LLM æµå¼ç”Ÿæˆ
        try:
            for event in self.llm.stream_chat(
                    query=user_prompt_content, # å°†æ¸²æŸ“åçš„ user_prompt å†…å®¹ä½œä¸º query ä¼ å…¥
                    system_prompt=system_prompt,
                    history=initial_state.get("history", [])
            ):
                if event["type"] == "content":
                    yield {"type": "delta", "content": event["data"]}
                elif event["type"] == "usage":
                    yield {"type": "usage", "usage": event["data"]}
        except Exception as e:
            logger.error(f"LLM Stream Error in Generation: {e}")
            yield {"type": "error", "content": str(e)}