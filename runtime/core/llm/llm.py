from openai import OpenAI
from config import Config
import logging

logger = logging.getLogger(__name__)

class LLMClient:
    def __init__(self):
        self.client = OpenAI(
            api_key=Config.DEEPSEEK_API_KEY,
            base_url=Config.DEEPSEEK_BASE_URL
        )
        self.model_name = "deepseek-chat" # æˆ–ä» Config è¯»å–

    def stream_chat(self, query: str, system_prompt: str, history: list = None):
        """
        æµå¼å¯¹è¯
        :param history: æ ¼å¼ [{"role": "user", "content": "..."}]
        """
        messages = []

        # 1. æ·»åŠ  System Prompt
        if system_prompt:
            messages.append({"role": "system", "content": system_prompt})

        # 2. æ·»åŠ å†å²è®°å½• (é™åˆ¶æœ€è¿‘ 5 è½®ï¼Œé˜²æ­¢ Token æº¢å‡º)
        if history:
            # ç®€å•çš„è½¬æ¢é€»è¾‘ï¼Œç¡®ä¿æ ¼å¼æ­£ç¡®
            for msg in history[-10:]:
                # å…¼å®¹ proto çš„ Message å¯¹è±¡æˆ– dict
                role = getattr(msg, 'role', None) or msg.get('role')
                content = getattr(msg, 'content', None) or msg.get('content')
                if role and content:
                    messages.append({"role": role, "content": content})

        # 3. æ·»åŠ å½“å‰é—®é¢˜ (å¦‚æœ query å·²ç»åœ¨ prompts é‡Œäº†ï¼Œè¿™é‡Œå¯ä»¥ä¸åŠ ï¼Œå–å†³äº prompts ç­–ç•¥)
        messages.append({"role": "user", "content": query})

        try:
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=messages,
                stream=True,
                temperature=0.3,
                stream_options={"include_usage": True},
            )

            for chunk in response:
                # 1. å¤„ç†å†…å®¹å¢é‡
                if chunk.choices and chunk.choices[0].delta.content:
                    yield {
                        "type": "content",
                        "data": chunk.choices[0].delta.content
                    }
                # 2. ğŸ”¥ å¤„ç† Token ç»Ÿè®¡ (é€šå¸¸åœ¨æœ€åä¸€å—)
                if hasattr(chunk, 'usage') and chunk.usage:
                    yield {
                        "type": "usage",
                        "data": {
                            "prompt_tokens": chunk.usage.prompt_tokens,
                            "completion_tokens": chunk.usage.completion_tokens,
                            "total_tokens": chunk.usage.total_tokens
                        }
                    }

        except Exception as e:
            logger.error(f"OpenAI API Error: {e}")
            raise e