import json
import time
import uuid
import glob
import logging
import hashlib
import traceback
from typing import Generator, Dict, Any, List, Optional

from core.llm.embedding import EmbeddingModel
from core.stores.qdrant_store import QdrantStore
from core.managers.kg_registry import KGRegistry
from core.connectors.base import ConnectorFactory

logger = logging.getLogger(__name__)

class ETLManager:
    def __init__(self, qdrant_store: QdrantStore, nebula_store: Any = None):
        self.qdrant = qdrant_store
        self.nebula = nebula_store
        self.embed_model = EmbeddingModel.get_instance()

        self.use_kg = self.nebula is not None and KGRegistry.is_active()

    def __del__(self):
        import glob
        temp_files = glob.glob("/tmp/chimera_img_*") + glob.glob("/tmp/chimera_table_*")
        for f in temp_files:
            try: os.remove(f)
            except: pass
        logger.info("ğŸ§¹ Temporary vision files cleaned up.")

    def sync_datasource(self, kb_id: int, source_id: int, source_type: str, config_json: str) -> Generator[Dict[str, Any], None, None]:
        """
        åŒæ­¥ä¸»ä»»åŠ¡ï¼šé›†æˆé¢†åŸŸæ„ŸçŸ¥ã€å¼‚æ­¥æ‰¹å¤„ç†ä¸çŠ¶æ€è‡ªæ„ˆ
        """
        start_time = time.time()
        logger.info(f"ğŸ”„ [ETL Start] KB={kb_id} Source={source_id} Type={source_type}")
        final_metrics = {
            "total_entities": 0,
            "linked_entities": 0,
            "visual_entities": 0,
            "total_chunks": 0
        }

        try:
            config = json.loads(config_json)
            connector_cls = ConnectorFactory.get_connector(source_type)
            connector = connector_cls(kb_id, source_id, config)

            # ç¼“å†²åŒºé…ç½®
            vector_buffer = []
            kg_batch_buffer = []
            V_BATCH_SIZE = 10
            K_BATCH_SIZE = 1 # é’ˆå¯¹ A4000 çš„ VLM ç¨³å®šæ€§ï¼Œå»ºè®®è®¾ä¸º 1 æˆ– 2

            total_processed = 0
            doc_domain = "general"

            # 1. é¢†åŸŸæ„ŸçŸ¥ï¼šé¢„è¯»ç¬¬ä¸€ä¸ªåˆ†ç‰‡
            classifier = KGRegistry.get_agent("classifier")
            chunks_iterator = connector.load()
            first_chunk = next(chunks_iterator, None)

            if first_chunk and classifier:
                classification = classifier.run(config.get("file_name", "Unknown"), first_chunk.content)
                doc_domain = classification.get("domain", "general")
                logger.info(f"ğŸ·ï¸  [Domain] æ–‡æ¡£é¢†åŸŸè¯†åˆ«ä¸º: {doc_domain.upper()}")

            # 2. å¤„ç†ç¬¬ä¸€ä¸ªåˆ†ç‰‡
            if first_chunk:
                self._process_single_chunk(first_chunk, kb_id, source_id, doc_domain, vector_buffer, kg_batch_buffer)

            # 3. å¾ªç¯å¤„ç†å‰©ä½™åˆ†ç‰‡
            for chunk in chunks_iterator:
                self._process_single_chunk(chunk, kb_id, source_id, doc_domain, vector_buffer, kg_batch_buffer)

                # 4. åˆ·æ–°é€»è¾‘ï¼šå‘é‡ä¼˜å…ˆåŸåˆ™ (é˜²æ­¢å›¾è°±æ›´æ–°æ—¶ ID ä¸å­˜åœ¨)
                if len(vector_buffer) >= V_BATCH_SIZE:
                    self.qdrant.upsert_chunks(vector_buffer)
                    vector_buffer = []

                if len(kg_batch_buffer) >= K_BATCH_SIZE:
                    # åœ¨æŠ½å›¾è°±å‰ï¼Œå¼ºåˆ¶æ’ç©ºå½“å‰çš„å‘é‡ç¼“å†²åŒº
                    batch_metrics = self._flush_kg_batch(kg_batch_buffer, domain=doc_domain)
                    for k in final_metrics:
                        if k in batch_metrics: final_metrics[k] += batch_metrics[k]
                    kg_batch_buffer = []


                final_metrics["total_chunks"] += 1
                yield {"chunks": final_metrics["total_chunks"], "status": "processing"}

            # 5. æ¸…ç†æœ€åæ®‹ç•™çš„ç¼“å†²åŒº
            if vector_buffer:
                self.qdrant.upsert_chunks(vector_buffer)
            if kg_batch_buffer:
                self._flush_kg_batch(kg_batch_buffer, domain=doc_domain)

            logger.info(f"âœ… [ETL Done] å…±å¤„ç† {total_processed} ä¸ªåˆ‡ç‰‡ï¼Œè€—æ—¶ {time.time() - start_time:.2f}s")
            yield {"success": True, "chunks": total_processed, "metrics": final_metrics}

        except Exception as e:
            logger.error(f"âŒ [ETL Error] {str(e)}")
            logger.error(traceback.format_exc())
            raise e
        finally:
            # ğŸ”¥ 4.1 è‡ªåŠ¨æ¸…ç†ä¸´æ—¶è§†è§‰æ–‡ä»¶
            for f in glob.glob("/tmp/chimera_img_*") + glob.glob("/tmp/chimera_table_*"):
                try: os.remove(f)
                except: pass

    def _process_single_chunk(self, chunk, kb_id, source_id, domain, v_buf, k_buf):
        """
        å†…éƒ¨é€»è¾‘å•å…ƒï¼šè´Ÿè´£å•ä¸ªåˆ‡ç‰‡çš„ VLM å¢å¼ºã€å‘é‡åŒ–å’ŒæŒ‡çº¹æ ¡éªŒ
        """
        chunk_uuid = str(uuid.uuid4())
        content_hash = chunk.metadata.get("content_hash")
        is_table = chunk.metadata.get("is_table", False)
        image_path = chunk.metadata.get("image_path")

        text_to_encode = chunk.content

        # 1. è§†è§‰å¢å¼º (VLM)
        # å¦‚æœæ˜¯è¡¨æ ¼ä¸”æœ‰æˆªå›¾ï¼Œæˆ–è€…æ˜¯ä¸€ä¸ª PICTURE
        if (is_table or image_path) and self.use_kg:
            try:
                from skills.vlm_service import VLMService
                vlm = VLMService.get_instance()

                # è°ƒç”¨ VLMï¼Œå¦‚æœæ˜¯è¡¨æ ¼åˆ™å¼€å¯ is_table æ¨¡å¼
                # è¿™é‡Œçš„ image_path å¯èƒ½æ˜¯ doc_parser ç”Ÿæˆçš„è¡¨æ ¼æˆªå›¾
                v_desc = vlm.describe_image(
                    image_path,
                    context_breadcrumb=chunk.metadata.get("breadcrumb", ""),
                    is_table=is_table
                )

                # æ‰§è¡Œè§†è§‰æ¨ç†
                image_desc = vlm.describe_image(image_path, context_breadcrumb=breadcrumb)

                # å°†è§†è§‰ä¿¡æ¯é”šå®šåˆ°æ–‡æœ¬ï¼Œç¡®ä¿â€œå›¾ç‰‡â€æœ¬èº«èƒ½è¢«æœç´¢åˆ°
                text_to_encode = f"ã€æ–‡æ¡£å›¾è¡¨è¯¦æƒ…ã€‘\n{v_desc}\n\n[æ£€ç´¢é”šç‚¹: {chunk.content}]"

                # ä»»åŠ¡å®Œæˆåç«‹å³æ¸…ç†ä¸´æ—¶å›¾ç‰‡æ–‡ä»¶ï¼Œé˜²æ­¢ç£ç›˜æº¢å‡º
                if os.path.exists(image_path):
                    os.remove(image_path)
            except Exception as ve:
                logger.error(f"âš ï¸ VLM è§£æå¤±è´¥: {ve}")

        # 2. ç”ŸæˆåµŒå…¥å‘é‡
        vector = self.embed_model.encode(text_to_encode)

        # 3. è£…è½½å‘é‡ç¼“å†²åŒº (æ³¨æ„ï¼šæ­¤å¤„å·²ä¿®å¤å˜é‡å)
        v_buf.append({
            "id": chunk_uuid,
            "vector": vector,
            "payload": {
                "content": text_to_encode,
                "kb_id": kb_id,
                "source_id": source_id,
                "content_hash": content_hash,
                "kg_status": "pending", # åˆå§‹çŠ¶æ€ä¸ºå¾…å®š
                "domain": domain,
                **{k: v for k, v in chunk.metadata.items() if k != 'image_path'}
            }
        })

        # 4. è£…è½½å›¾è°±ç¼“å†²åŒº (å¢é‡æ ¡éªŒ)
        if self.use_kg:
            if not self._check_kg_completed(content_hash):
                k_buf.append({
                    "id": chunk_uuid,
                    "text": text_to_encode,
                    "metadata": chunk.metadata
                })
            else:
                # è®°å½•è·³è¿‡æ—¥å¿—ï¼Œç”¨äºç›‘æ§å¢é‡åŒæ­¥æ•ˆç‡
                logger.info(f"â­ï¸  [KG-Skip] å†…å®¹æŒ‡çº¹ {content_hash[:8]} å·²å­˜åœ¨ï¼Œè·³è¿‡ LLM æŠ½å–ã€‚")

    def _check_kg_completed(self, content_hash):
        if not content_hash: return False
        try:
            from qdrant_client.http import models
            res = self.client.scroll(
                collection_name=self.qdrant.collection_name,
                scroll_filter=models.Filter(must=[
                    models.FieldCondition(key="content_hash", match=models.MatchValue(value=content_hash)),
                    models.FieldCondition(key="kg_status", match=models.MatchValue(value="completed"))
                ]), limit=1
            )
            return len(res[0]) > 0
        except: return False

    def _flush_kg_batch(self, buffer: List[Dict], domain: str = "general"):
        """
        æ‰¹é‡æŠ½å–å¹¶å…¥åº“ï¼ŒæˆåŠŸåæ›´æ–° Qdrant çŠ¶æ€
        é›†æˆè§†è§‰é€»è¾‘åŒ–æŠ½å–
        """
        stats = {"visual_extracted": 0, "entities_linked": 0, "new_entities": 0}
        extractor = KGRegistry.get_agent("extractor")
        inspector = KGRegistry.get_agent("inspector")
        resolver = KGRegistry.get_agent("resolution")
        if not extractor: return

        # æ‡’åŠ è½½ VLMServiceï¼Œåªæœ‰åœ¨éœ€è¦æ—¶æ‰å ç”¨æ˜¾å­˜
        from skills.vlm_service import VLMService

        logger.info(f"ğŸ“¦ [KG-Batch] å¼€å§‹å¤„ç† {len(buffer)} ä¸ªåˆ‡ç‰‡...")

        # --- æ­¥éª¤ A: è§†è§‰å¢å¼ºï¼ˆé’ˆå¯¹å«æœ‰å›¾ç‰‡çš„åˆ‡ç‰‡ï¼‰ ---
        processed_items = []
        for item in buffer:
            text_content = item["text"]
            # æ£€æŸ¥ metadata ä¸­æ˜¯å¦å­˜æœ‰ä¸´æ—¶å›¾ç‰‡è·¯å¾„ (ç”± doc_parser ç”Ÿæˆ)
            image_path = item.get("metadata", {}).get("image_path")

            if image_path and os.path.exists(image_path):
                try:
                    logger.info(f"ğŸ‘ï¸  [VLM] æ¢æµ‹åˆ°æ¶æ„å›¾/æ’å›¾ï¼Œå¯åŠ¨ A4000 è§†è§‰è¯†åˆ«...")
                    vlm = VLMService.get_instance()
                    # è°ƒå–æˆ‘ä»¬åœ¨ 2.1 è·‘é€šçš„æè¿°æ–¹æ³•
                    image_desc = vlm.describe_image(image_path)

                    # ğŸ”¥ æ ¸å¿ƒï¼šå°†è§†è§‰é€»è¾‘èå…¥æ–‡æœ¬
                    text_content += f"\n\nã€å›¾ç‰‡è§†è§‰é€»è¾‘æè¿°ã€‘: {image_desc}"
                    logger.info(f"âœ… [VLM] è¯†åˆ«å®Œæˆï¼Œæè¿°å­—æ•°: {len(image_desc)}")
                except Exception as ve:
                    logger.error(f"âš ï¸ [VLM] è§†è§‰è§£æè·³è¿‡: {ve}")

            item["text"] = text_content
            processed_items.append(item)

        # --- æ­¥éª¤ B: æ‰§è¡ŒåŸæœ‰çš„å›¾è°±æŠ½å–æµç¨‹ ---
        try:
            processed_buffer = []
            for item in buffer:
                enriched_text = item["text"]

                # å¦‚æœè¯¥åˆ‡ç‰‡å¸¦æœ‰å›¾ç‰‡å­—èŠ‚
                if item.get("metadata", {}).get("image_bytes"):
                    logger.info(f"ğŸ‘ï¸  [VLM] æ¢æµ‹åˆ°å›¾ç‰‡ï¼Œå¯åŠ¨ A4000 è§†è§‰è¯†åˆ«: {item['id'][:8]}...")
                    from skills.vlm_service import VLMService
                    vlm = VLMService.get_instance()

                    # è°ƒç”¨ A4000 è¿è¡Œ Qwen2-VL
                    image_desc = vlm.describe_image(item["metadata"]["image_bytes"])

                    # æ ¸å¿ƒæ“ä½œï¼šå°†è§†è§‰æè¿°æ‹¼æ¥è¿›æ–‡æœ¬ï¼Œå–‚ç»™åç»­çš„ ExtractorAgent
                    enriched_text = f"{enriched_text}\nã€è§†è§‰è¡¥å……æè¿°ã€‘: {image_desc}"

                item["text"] = enriched_text
                processed_buffer.append(item)
            # 1. æ‰§è¡Œæ‰¹é‡ LLM æŠ½å–
            batch_data = extractor.run_batch(processed_buffer, domain=domain)
            results = batch_data.get("results", [])
            successful_ids = []

            for i, res in enumerate(results):
                # ğŸ”¥ 2.3 å¢å¼ºï¼šå¦‚æœå½“å‰åˆ‡ç‰‡æ˜¯è¡¨æ ¼ï¼Œå¼ºè¡Œæ³¨å…¥ä¸€ä¸ªâ€œè¡¨æ ¼å®ä½“â€
                # è¿™æ · Resolver å°±èƒ½æŠŠæ–‡å­—å¼•ç”¨çš„ Table_1 å’Œè¿™ä¸ªå®ä½“å¯¹é½
                if processed_items[i].get("metadata", {}).get("is_table"):
                    table_label = "è¡¨æ ¼" # é€»è¾‘ä¸Šå¯ä»¥ä» content æå–æ›´ç»†çš„æ ‡è¯†
                    res["entities"].append({
                        "name": table_label,
                        "type": "Table_Object",
                        "desc": "æ–‡æ¡£ä¸­çš„ç»“æ„åŒ–æ•°æ®è¡¨"
                    })

            successful_chunk_ids = []

            for i, res in enumerate(results):
                if i >= len(buffer): break
                # 1. æ£€ç´¢å…¨å±€å­˜é‡
                global_refs = []
                if self.nebula and hasattr(self.nebula, 'es_store') and self.nebula.es_store:
                    for ent in res.get('entities', []):
                        vids = self.nebula.es_store.search_entities(ent['name'], top_k=1)
                        for v in vids:
                            old = self.nebula.get_entity_detail(v)
                            if old: global_refs.append(old)

                # 2. è´¨é‡å®¡è®¡ä¸æ¶ˆè§£
                refined_kb = inspector.run(buffer[i]["text"], res)
                # ğŸ”¥ ä¼ å…¥å…¨å±€å‚è€ƒ
                res_out = resolver.run(refined_kb.get('entities', []), refined_kb.get('relations', []), global_ref=global_refs)

                # 3. ç»Ÿè®¡
                m = res_out.get("metrics", {})
                metrics["total_entities"] += m.get("total_extracted", 0)
                metrics["entities_linked"] += m.get("linked_count", 0)
                if buffer[i].get("is_visual"):
                    metrics["visual_entities"] += m.get("total_extracted", 0)

                self.nebula.upsert_graph(res_out, buffer[i]["id"])
                successful_ids.append(buffer[i]["id"])

            if successful_ids: self._mark_kg_success_in_qdrant(successful_ids)
        except Exception as e:
            logger.error(f"Batch Failed: {e}")
        return metrics

    def _mark_kg_success_in_qdrant(self, chunk_ids: List[str]):
        """
        å¼ºåˆ¶æ›´æ–° Qdrant çŠ¶æ€ä½ä¸º completed
        """
        try:
            # æ‰¹é‡æ›´æ–°æé«˜æ•ˆç‡
            for cid in chunk_ids:
                self.qdrant.client.set_payload(self.qdrant.collection_name,
                                               {"kg_status": "completed"},
                                               [cid],
                                               wait=False)
            logger.info(f"âœ… å·²æ›´æ–° {len(chunk_ids)} ä¸ªåˆ‡ç‰‡çš„å›¾è°±çŠ¶æ€ä¸º completed")
        except Exception as e:
            logger.error(f"âŒ æ›´æ–°çŠ¶æ€ä½å¤±è´¥: {e}")
