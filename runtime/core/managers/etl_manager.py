import json
import time
import uuid
import logging
import traceback
from typing import Generator, Dict, Any, Optional

from core.llm.embedding import EmbeddingModel
from core.stores.qdrant_store import QdrantStore
from core.connectors.base import ConnectorFactory
from core.managers.kg_registry import KGRegistry

logger = logging.getLogger(__name__)

class ETLManager:
    def __init__(self, qdrant_store: QdrantStore, nebula_store: Any = None):
        self.qdrant = qdrant_store
        self.nebula = nebula_store
        self.embed_model = EmbeddingModel.get_instance()
        self.is_kg_active = False

        if self.nebula:
            self._init_kg_status()

    def _init_kg_status(self):
        """
        ä¸å†éœ€è¦ try-importï¼Œç›´æ¥æ£€æŸ¥æ³¨å†Œè¡¨
        """
        if KGRegistry.is_active():
            self.is_kg_active = True
            logger.info("ğŸ”“ [ETL] Enterprise GraphRAG Pipeline linked via Registry.")
        else:
            logger.info("â„¹ï¸ [ETL] KG Agents not registered. Skipping KG construction.")
            self.nebula = None

    def sync_datasource(self, kb_id: int, source_id: int, source_type: str, config_json: str) -> Generator[Dict[str, Any], None, None]:
        """
        æ‰§è¡ŒåŒæ­¥ä»»åŠ¡ï¼šå‘é‡å…¥åº“ + (å¯é€‰) å›¾è°±å…¥åº“
        """
        start_time = time.time()
        logger.info(f"ğŸ”„ [ETL Start] KB={kb_id} Source={source_id} Type={source_type}")

        try:
            config = json.loads(config_json)
            connector_cls = ConnectorFactory.get_connector(source_type)
            if not connector_cls:
                raise ValueError(f"Unsupported connector: {source_type}")

            connector = connector_cls(kb_id, source_id, config)
            chunks_buffer = []
            total_count = 0

            for chunk in connector.load():
                chunk_uuid = str(uuid.uuid4())
                vector = self.embed_model.encode(chunk.content)

                payload = {
                    "id": chunk_uuid,
                    "vector": vector,
                    "payload": {
                        "content": chunk.content,
                        "kb_id": kb_id,
                        "source_id": source_id,
                        **chunk.metadata
                    }
                }
                chunks_buffer.append(payload)

                # --- è¿è¡Œå›¾è°±æµæ°´çº¿ ---
                if self.nebula and self.is_kg_active:
                    self._run_kg_pipeline_safe(chunk, chunk_uuid)

                # --- æ‰¹é‡å†™å…¥ Qdrant (æ¯ 10 æ¡) ---
                if len(chunks_buffer) >= 10:
                    self.qdrant.upsert_chunks(chunks_buffer)
                    total_count += len(chunks_buffer)
                    chunks_buffer = []
                    yield {"chunks": total_count, "status": "syncing"}

            # å†™å…¥å‰©ä½™éƒ¨åˆ†
            if chunks_buffer:
                self.qdrant.upsert_chunks(chunks_buffer)
                total_count += len(chunks_buffer)

            logger.info(f"ğŸ’¾ æ€»è®¡å‘ Qdrant å†™å…¥ {total_count} æ¡å‘é‡æ•°æ®")
            yield {"success": True, "chunks": total_count}

        except Exception as e:
            logger.error(f"âŒ [ETL Error] {str(e)}")
            logger.error(traceback.format_exc())
            raise e

    def _run_kg_pipeline_safe(self, chunk, chunk_id: str):
        """
        æ‰§è¡Œæµæ°´çº¿æ—¶ï¼Œç›´æ¥ä»æ³¨å†Œè¡¨å– Agent
        """
        if not self.nebula or not KGRegistry.is_active():
            return

        try:
            logger.info(f"ğŸš€ [KG-Pipeline] å¼€å§‹å¤„ç†åˆ‡ç‰‡: {chunk_id[:8]}...")
            # ä»æ³¨å†Œè¡¨åŠ¨æ€è·å– Agent å®ä¾‹
            extractor = KGRegistry.get_agent("extractor")
            inspector = KGRegistry.get_agent("inspector")
            resolver = KGRegistry.get_agent("resolution")
            es_indexer = KGRegistry.get_agent("es_indexer")
            if es_indexer:
                for ent in final_kb['entities']:
                    es_indexer.index_entity(ent['name'], vid)

            if not all([extractor, inspector, resolver]):
                return

            # A. è”åˆæŠ½å–
            breadcrumb = chunk.metadata.get("breadcrumb", "")
            raw_kb = extractor.run(chunk.content, breadcrumb)

            # B. è´¨é‡å®¡æŸ¥
            refined_kb = inspector.run(chunk.content, raw_kb)

            # C. æ¢¯åº¦æ¶ˆæ­§
            final_kb = resolver.run(refined_kb.get('entities', []), refined_kb.get('relations', []))

            # D. å…¥åº“
            self.nebula.upsert_graph(final_kb, chunk_id)

        except Exception as ex:
            logger.warning(f"âš ï¸ [KG Pipeline] Failed: {ex}")