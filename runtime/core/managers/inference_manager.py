import json
import time
import logging
import traceback
from typing import Generator, Dict, Any, List

from opentelemetry import trace
from workflows.chat_flow import ChatWorkflow

logger = logging.getLogger(__name__)
tracer = trace.get_tracer(__name__)

class InferenceManager:
    def __init__(self, qdrant_store, nebula_store=None):
        """
        åˆå§‹åŒ–æ¨ç†ç®¡ç†å™¨
        :param qdrant_store: å‘é‡å­˜å‚¨ (Core)
        :param nebula_store: å›¾å­˜å‚¨ (Enterprise, å¯é€‰)
        """
        self.qdrant = qdrant_store
        self.nebula = nebula_store

    def run_chat(self, query: str, history: List[Any], app_config_json: str) -> Generator[Dict[str, Any], None, None]:
        """
        æ‰§è¡Œå¯¹è¯å·¥ä½œæµ
        :param query: ç”¨æˆ·é—®é¢˜
        :param history: å†å²è®°å½• (gRPC Message list)
        :param app_config_json: åº”ç”¨é…ç½® (å« kb_ids, org_id)
        :yield: æ ‡å‡†åŒ–çš„äº‹ä»¶å­—å…¸ (type, payload, meta)
        """
        start_time = time.time()

        # 1. å‡†å¤‡ç»Ÿè®¡æ•°æ®
        usage_stats = {
            "prompt_tokens": 0,
            "completion_tokens": 0,
            "total_tokens": 0
        }

        # è·å–å½“å‰ TraceID (ç”¨äºè¿”å›ç»™å‰ç«¯å±•ç¤º)
        current_span = trace.get_current_span()
        trace_id = format(current_span.get_span_context().trace_id, "032x")

        try:
            # 2. è§£æé…ç½®
            app_config = json.loads(app_config_json)
            kb_ids = app_config.get("kb_ids", [])

            # 3. åˆå§‹åŒ–å·¥ä½œæµ (æ¯æ¬¡è¯·æ±‚å¯èƒ½é’ˆå¯¹ä¸åŒçš„ KBï¼Œæ‰€ä»¥åœ¨è¿™é‡Œåˆå§‹åŒ–)
            # æ³¨æ„ï¼šChatWorkflow å†…éƒ¨å·²ç»åšäº†å¯¹ nebula ä¸º None çš„å®¹é”™å¤„ç† (è§ Phase 1 æ­¥éª¤ 4)
            workflow = ChatWorkflow(self.nebula, self.qdrant, kb_ids)

            # 4. æ„é€ åˆå§‹çŠ¶æ€
            initial_state = {
                "query": query,
                "history": history,
                "app_config": app_config
            }

            # 5. æ‰§è¡Œå·¥ä½œæµå¹¶å¤„ç†æµå¼äº‹ä»¶
            for event in workflow.run_stream(initial_state):

                # A. æ€è€ƒ/æ¨ç†è¿‡ç¨‹
                if event["type"] == "thought":
                    yield {
                        "type": "thought",
                        "payload": event["content"],
                        "meta": {
                            "node_name": event.get("node", "Agent"),
                            "trace_id": trace_id,
                            "duration_ms": event.get("duration", 0)
                        }
                    }

                # B. ç­”æ¡ˆç‰‡æ®µ
                elif event["type"] == "delta":
                    yield {
                        "type": "delta",
                        "payload": event["content"]
                    }

                # C. å¼•ç”¨æ–‡æ¡£
                elif event["type"] == "reference":
                    yield {
                        "type": "reference",
                        "payload": json.dumps(event["docs"]) # åºåˆ—åŒ–åè¿”å›
                    }

                # D. Token ç»Ÿè®¡
                elif event["type"] == "usage":
                    u = event["usage"]
                    usage_stats["prompt_tokens"] += u.get("prompt_tokens", 0)
                    usage_stats["completion_tokens"] += u.get("completion_tokens", 0)
                    usage_stats["total_tokens"] += u.get("total_tokens", 0)

                elif event["type"] == "subgraph":
                    yield {
                        "type": "subgraph",
                        "payload": event["payload"]
                    }

        except Exception as e:
            logger.error(f"âŒ [Inference] Error: {str(e)}")
            logger.error(traceback.format_exc())
            yield {
                "type": "error",
                "payload": f"Inference Error: {str(e)}"
            }

        finally:
            # 6. ç”Ÿæˆæœ€ç»ˆæ‘˜è¦ (Summary)
            duration = int((time.time() - start_time) * 1000)
            logger.info(f"ğŸ“Š [Inference Done] Tokens={usage_stats['total_tokens']} Time={duration}ms")

            yield {
                "type": "summary",
                "summary": {
                    "total_tokens": usage_stats["total_tokens"],
                    "prompt_tokens": usage_stats["prompt_tokens"],
                    "completion_tokens": usage_stats["completion_tokens"],
                    "total_duration_ms": duration,
                    "final_status": "success"
                }
            }