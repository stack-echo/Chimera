import json
import logging
import os
import yaml
from typing import TypedDict, List, Dict, Any, Generator, Optional
from langgraph.graph import StateGraph, END
from jinja2 import Template

from core.llm.embedding import EmbeddingModel
from core.llm.llm import LLMClient
from core.stores.qdrant_store import QdrantStore
# âŒ å·²åˆ é™¤: from core.stores.graph_store import NebulaStore (è¿™æ˜¯ä¼ä¸šç‰ˆç»„ä»¶ï¼Œä¸èƒ½åœ¨ Core ç›´æ¥å¼•å…¥)
from agents.chat.query_analysis import QueryAnalysisAgent

logger = logging.getLogger(__name__)

# --- çŠ¶æ€å®šä¹‰ ---
class AgentState(TypedDict):
    query: str
    chat_history: List[Dict[str, str]]
    query_entities: List[str]
    retrieved_docs: List[Dict]
    graph_context: List[str]
    answer: str

class ChatWorkflow:
    # ğŸ”¥ ä¿®æ”¹ç‚¹ï¼šnebula ç±»å‹æ”¹ä¸º Anyï¼Œå…è®¸ä¼ å…¥ None
    def __init__(self, nebula: Any, qdrant: QdrantStore, kb_ids: List[int]):
        self.nebula = nebula
        self.qdrant = qdrant
        self.kb_ids = kb_ids

        self.embed_model = EmbeddingModel.get_instance()
        self.llm = LLMClient()
        self.query_analyzer = QueryAnalysisAgent()

        # åŠ è½½ç”Ÿæˆ Prompt
        self.synthesis_prompt_config = self._load_prompt("chat/synthesis.yaml")
        self.app = self._build_graph()

    def _load_prompt(self, filename):
        base_dir = os.getcwd()
        # å…¼å®¹ä¸åŒå¯åŠ¨è·¯å¾„
        if "runtime" not in base_dir and os.path.exists("runtime"):
            base_dir = os.path.join(base_dir, "runtime")

        # å‡è®¾ prompts ç›®å½•åœ¨ runtime/prompts
        path = os.path.join(base_dir, "prompts", filename)

        if not os.path.exists(path):
            # å›é€€å°è¯• (å¤„ç† Docker è·¯å¾„å¯èƒ½ä¸åŒ)
            path = os.path.join("/app/prompts", filename)

        if not os.path.exists(path):
            # å†æ¬¡å›é€€ï¼Œé˜²æ­¢æœ¬åœ°è°ƒè¯•è·¯å¾„é—®é¢˜
            if os.path.exists(f"prompts/{filename}"):
                path = f"prompts/{filename}"
            else:
                # æœ€åçš„å…œåº•ï¼Œå¦‚æœæ˜¯ runtimeservice å¯åŠ¨
                path = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(__file__))), "prompts", filename)

        if not os.path.exists(path):
            raise FileNotFoundError(f"âŒ æç¤ºè¯æ–‡ä»¶æœªæ‰¾åˆ°: {path}")

        with open(path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)

    def _build_graph(self):
        workflow = StateGraph(AgentState)

        workflow.add_node("query_analysis", self.node_query_analysis)
        workflow.add_node("retrieve", self.node_retrieve)
        workflow.add_node("generate", self.node_generate)

        workflow.set_entry_point("query_analysis")
        workflow.add_edge("query_analysis", "retrieve")
        workflow.add_edge("retrieve", "generate")
        workflow.add_edge("generate", END)

        return workflow.compile()

    # --- èŠ‚ç‚¹é€»è¾‘ ---

    def node_query_analysis(self, state: AgentState):
        """æ­¥éª¤ 1: åˆ†æç”¨æˆ· Queryï¼Œæå–å…³é”®å®ä½“"""
        logger.info(f"ğŸ§  [Chat-1] Query Analysis: {state['query']}")
        entities = self.query_analyzer.run(state["query"])
        return {"query_entities": entities}

    def node_retrieve(self, state: AgentState):
        """æ­¥éª¤ 2: åŒè·¯æ£€ç´¢ (Vector + Graph)"""
        query = state["query"]
        entities = state.get("query_entities", [])
        if not entities: entities = [query]

        # A. å‘é‡æ£€ç´¢ (Core)
        vector_results = []
        try:
            query_vec = self.embed_model.encode(query)
            vector_results = self.qdrant.search(query_vec, self.kb_ids, top_k=3)
        except Exception as e:
            logger.error(f"Vector Search Error: {e}")

        # B. å›¾è°±æ£€ç´¢ (Enterprise)
        graph_triplets = []
        # ğŸ”¥ å…³é”®ä¿®æ”¹ï¼šå…ˆæ£€æŸ¥ self.nebula æ˜¯å¦å­˜åœ¨
        if self.nebula:
            try:
                # Duck Typing: åªè¦ä¼ å…¥çš„å¯¹è±¡æœ‰ retrieve_subgraph æ–¹æ³•å°±è¡Œ
                graph_triplets = self.nebula.retrieve_subgraph(entities)
                logger.info(f"ğŸ•¸ï¸ [Chat-2] KG Hit: {len(graph_triplets)} relations")
            except Exception as e:
                logger.error(f"Graph Search Error: {e}")
        else:
            logger.debug("ğŸ•¸ï¸ [Chat-2] Skipping KG (Enterprise feature disabled)")

        return {
            "retrieved_docs": vector_results,
            "graph_context": graph_triplets
        }

    def node_generate(self, state: AgentState):
        return {}

    # --- è¿è¡Œé€»è¾‘ ---

    def run_stream(self, initial_state: dict) -> Generator[Dict[str, Any], None, None]:
        final_state = self.app.invoke(initial_state)

        query = final_state["query"]
        vec_docs = final_state.get("retrieved_docs", [])
        graph_triplets = final_state.get("graph_context", [])

        # ç»„è£… Context
        doc_context_str = "\n".join([f"- {d['content']}" for d in vec_docs])
        if not doc_context_str: doc_context_str = "æ— ç›¸å…³æ–‡æ¡£ç‰‡æ®µã€‚"

        kg_context_str = "\n".join(graph_triplets)
        if not kg_context_str: kg_context_str = "æ— ç›¸å…³çŸ¥è¯†å›¾è°±ä¿¡æ¯ã€‚"

        full_context = f"ã€æ–‡æ¡£ç‰‡æ®µã€‘ï¼š\n{doc_context_str}\n\nã€çŸ¥è¯†å›¾è°±è·¯å¾„ã€‘ï¼š\n{kg_context_str}"

        # æ¸²æŸ“ Prompt
        sys_tmpl = self.synthesis_prompt_config.get("system", "")
        user_tmpl = self.synthesis_prompt_config.get("user", "")

        system_prompt = Template(sys_tmpl).render(full_context=full_context)
        user_prompt_content = Template(user_tmpl).render(query=query)

        # æµå¼ç”Ÿæˆ
        try:
            for event in self.llm.stream_chat(
                    query=user_prompt_content,
                    system_prompt=system_prompt,
                    history=initial_state.get("history", [])
            ):
                if event["type"] == "content":
                    yield {"type": "delta", "content": event["data"]}
                elif event["type"] == "usage":
                    yield {"type": "usage", "usage": event["data"]}
        except Exception as e:
            logger.error(f"LLM Generation Error: {e}")
            yield {"type": "error", "content": str(e)}