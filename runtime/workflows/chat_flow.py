import json
import logging
import os
import yaml
from typing import TypedDict, List, Dict, Any, Generator, Optional
from langgraph.graph import StateGraph, END
from jinja2 import Template

# Core & Skills
from core.llm.embedding import EmbeddingModel
from core.llm.llm import LLMClient
from core.stores.qdrant_store import QdrantStore
from skills.reranker import CognitiveReranker
from agents.chat.query_analysis import QueryAnalysisAgent
from core.telemetry.tracing import trace_agent

logger = logging.getLogger(__name__)

# --- 1. çŠ¶æ€å®šä¹‰ ---
class AgentState(TypedDict):
    query: str
    history: List[Any]              # åŸå§‹ gRPC Message å¯¹è±¡åˆ—è¡¨
    query_entities: List[str]       # æå–çš„å®ä½“/å…³é”®è¯
    retrieved_docs: List[Dict]      # ç»è¿‡ Skyline è¿‡æ»¤åçš„é»„é‡‘æ–‡æ¡£ç‰‡æ®µ
    graph_context: List[str]        # ç”¨äº Prompt æ³¨å…¥çš„å›¾è°±èƒŒæ™¯æè¿°
    subgraph_data: Dict[str, List]  # ç”¨äºå‰ç«¯å¯è§†åŒ–çš„ç‚¹è¾¹åŸå§‹æ•°æ®
    full_context: str               # æœ€ç»ˆæ‹¼è£…çš„ä¸Šä¸‹æ–‡å­—ç¬¦ä¸²
    answer: str                     # ç”Ÿæˆçš„ç»“æœ

class ChatWorkflow:
    def __init__(self, nebula: Any, qdrant: QdrantStore, kb_ids: List[int]):
        """
        :param nebula: ä¼ä¸šç‰ˆ NebulaStore å®ä¾‹æˆ– None
        :param qdrant: QdrantStore å®ä¾‹
        :param kb_ids: çŸ¥è¯†åº“ ID åˆ—è¡¨
        """
        self.nebula = nebula
        self.qdrant = qdrant
        self.kb_ids = kb_ids

        self.embed_model = EmbeddingModel.get_instance()
        self.llm = LLMClient()
        self.query_analyzer = QueryAnalysisAgent()

        # åŠ è½½ç”Ÿæˆ Prompt
        self.synthesis_prompt_config = self._load_prompt("chat/synthesis.yaml")
        # æ„å»ºå›¾
        self.app = self._build_graph()

    def _load_prompt(self, filename):
        """å¢å¼ºçš„æç¤ºè¯åŠ è½½é€»è¾‘ï¼Œæ”¯æŒå¤šè·¯å¾„æœç´¢"""
        base_dir = os.getcwd()
        paths = [
            os.path.join(base_dir, "runtime/prompts", filename),
            os.path.join(base_dir, "prompts", filename),
            os.path.join("/app/prompts", filename)
        ]
        for path in paths:
            if os.path.exists(path):
                with open(path, 'r', encoding='utf-8') as f:
                    return yaml.safe_load(f)
        raise FileNotFoundError(f"âŒ Prompt file {filename} not found.")

    def _build_graph(self):
        """æ„å»º LangGraph å·¥ä½œæµ"""
        workflow = StateGraph(AgentState)

        workflow.add_node("query_analysis", self.node_query_analysis)
        workflow.add_node("retrieve", self.node_retrieve)
        workflow.add_node("generate_prep", self.node_generate_prep)

        workflow.set_entry_point("query_analysis")
        workflow.add_edge("query_analysis", "retrieve")
        workflow.add_edge("retrieve", "generate_prep")
        workflow.add_edge("generate_prep", END)

        return workflow.compile()

    # --- 2. èŠ‚ç‚¹é€»è¾‘ (Nodes) ---

    @trace_agent("Node:Query_Analysis")
    def node_query_analysis(self, state: AgentState):
        """æ­¥éª¤ 1: æå–å…³é”®è¯å¹¶è¿›è¡Œæ„å›¾é”šå®š"""
        logger.info(f"ğŸ§  [Chat-1] åˆ†ææ„å›¾: {state['query']}")
        entities = self.query_analyzer.run(state["query"])
        return {"query_entities": entities}

    @trace_agent("Node:Dual_Retrieval")
    def node_retrieve(self, state: AgentState):
        """æ­¥éª¤ 2: åŒèºæ—‹æ£€ç´¢ + å¤šç»´ Skyline è¿‡æ»¤"""
        query = state["query"]
        entities = state.get("query_entities", [])

        # 2.1 åˆå§‹åŒ–å®¹å™¨
        graph_context = []
        graph_chunk_hits = {}
        subgraph_raw = {"nodes": [], "edges": []}

        # 2.2 ä¼ä¸šç‰ˆå›¾è°±æ”¯æµ (Enterprise)
        if self.nebula:
            try:
                # Stage-1: è·å–å›¾è°±èƒŒæ™¯æ–‡æœ¬ (Cog-RAG)
                graph_context = self.nebula.retrieve_topic_context(entities)
                # è·å–å›¾è°±è¯„åˆ† (ç”¨äº Skyline è¿‡æ»¤)
                graph_chunk_hits = self.nebula.get_chunk_scores_by_entities(entities)
                # ä»»åŠ¡ 4.1: è·å–å¯è§†åŒ–åŸå§‹ç‚¹è¾¹
                subgraph_raw = self.nebula.get_subgraph_raw(entities)
                logger.info(f"ğŸ•¸ï¸ [Chat-2] å›¾è°±å‘½ä¸­äº† {len(graph_context)} ä¸ªèƒŒæ™¯äº‹å®")
            except Exception as e:
                logger.error(f"âš ï¸ Nebula Retrieval Error: {e}")

        # 2.3 å¼€æºç‰ˆå‘é‡æ”¯æµ (Core)
        query_vec = self.embed_model.encode(query)
        # å¬å›å€™é€‰é›† (Top-25)ï¼Œä¾› Skyline ç®—æ³•ç²¾é€‰
        raw_vector_hits = self.qdrant.search(query_vec, self.kb_ids, top_k=25)

        # 2.4 å¤šç»´ Skyline è¿‡æ»¤ (Task 3.3)
        refined_docs = CognitiveReranker.skyline_filter(
            vector_results=raw_vector_hits,
            graph_scores=graph_chunk_hits,
            top_k=7
        )

        return {
            "retrieved_docs": refined_docs,
            "graph_context": graph_context,
            "subgraph_data": subgraph_raw
        }

    @trace_agent("Node:Context_Fusion")
    def node_generate_prep(self, state: AgentState):
        """æ­¥éª¤ 3: è®¤çŸ¥èåˆä¸Šä¸‹æ–‡æ‹¼è£…"""
        vec_docs = state.get("retrieved_docs", [])
        graph_data = state.get("graph_context", [])

        # A. æ ¼å¼åŒ–å›¾è°±äº‹å®
        kg_section = ""
        if graph_data:
            kg_section = "ã€çŸ¥è¯†å›¾è°±èƒŒæ™¯ä¿¡æ¯ï¼ˆæ ¸å¿ƒäº‹å®ï¼‰ã€‘:\n" + "\n".join([f"- {t}" for t in graph_data])

        # B. æ ¼å¼åŒ–æ–‡æ¡£ç‰‡æ®µ (åŒ…å« Breadcrumb å±‚æ¬¡ä¿¡æ¯)
        doc_parts = []
        for i, d in enumerate(vec_docs):
            content = d.get('content', '')
            # è¿™é‡Œçš„ content å·²ç»ç»è¿‡ 1.2 ä»»åŠ¡çš„ DoclingParser å¤„ç†ï¼ŒåŒ…å«äº†ç« èŠ‚è·¯å¾„
            source = d.get('metadata', {}).get('file_name', 'æœªçŸ¥æ–‡æ¡£')
            page = d.get('metadata', {}).get('page_number', '?')
            doc_parts.append(f"è¯æ®[{i+1}] (æ¥æº: {source}, é¡µç : {page})\n{content}")

        doc_section = "ã€ç›¸å…³æ–‡æ¡£è¯¦æƒ…ï¼ˆè¡¥å……ç»†èŠ‚ï¼‰ã€‘:\n" + "\n\n".join(doc_parts)

        # æ‹¼è£…æœ€ç»ˆç»™ LLM çš„ä¸Šä¸‹æ–‡
        full_context = f"{kg_section}\n\n{doc_section}".strip()
        if not full_context:
            full_context = "çŸ¥è¯†åº“ä¸­æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚"

        return {"full_context": full_context}

    # --- 3. è¿è¡Œé€»è¾‘ (Stream Handling) ---

    def run_stream(self, initial_state: dict) -> Generator[Dict[str, Any], None, None]:
        """
        æ‰§è¡Œå·¥ä½œæµå¹¶äº§ç”Ÿæ ‡å‡†åŒ–äº‹ä»¶æµ
        """
        # 1. æ‰§è¡Œå›¾é€»è¾‘ï¼ˆåŒæ­¥è°ƒç”¨ï¼Œç›´åˆ° generate_prep ç»“æŸï¼‰
        final_state = self.app.invoke(initial_state)

        # 2. æ¨é€ä¸­é—´æ€è€ƒè¿‡ç¨‹ï¼ˆ thought ï¼‰ç»™å‰ç«¯
        if final_state.get("query_entities"):
            yield {
                "type": "thought",
                "node": "QueryAnalysis",
                "content": f"æ­£åœ¨æ£€ç´¢å®ä½“: {', '.join(final_state['query_entities'])}"
            }

        # 3. æ¨é€ä»»åŠ¡ 4.1 å­å›¾æ•°æ® (ç”¨äº ECharts ç»˜å›¾)
        if final_state.get("subgraph_data") and final_state["subgraph_data"].get("nodes"):
            yield {
                "type": "subgraph",
                "payload": json.dumps(final_state["subgraph_data"], ensure_ascii=False)
            }

        # 4. æ¨é€å‚è€ƒå¼•ç”¨ (reference)
        if final_state.get("retrieved_docs"):
            yield {
                "type": "reference",
                "docs": final_state["retrieved_docs"]
            }

        # 5. è°ƒç”¨ LLM è¿›è¡Œæœ€ç»ˆç”Ÿæˆ (LLM Stream)
        sys_tmpl = self.synthesis_prompt_config.get("system", "")
        user_tmpl = self.synthesis_prompt_config.get("user", "")

        # æ³¨å…¥ç”± generate_prep å‡†å¤‡å¥½çš„ä¸Šä¸‹æ–‡
        system_prompt = Template(sys_tmpl).render(full_context=final_state["full_context"])
        user_prompt_content = Template(user_tmpl).render(query=final_state["query"])

        try:
            for event in self.llm.stream_chat(
                    query=user_prompt_content,
                    system_prompt=system_prompt,
                    history=initial_state.get("history", []) # é€ä¼ å†å²è®°å½•
            ):
                if event["type"] == "content":
                    yield {"type": "delta", "content": event["data"]}
                elif event["type"] == "usage":
                    yield {"type": "usage", "usage": event["data"]}
        except Exception as e:
            logger.error(f"âŒ LLM Generation Failed: {e}")
            yield {"type": "error", "content": str(e)}