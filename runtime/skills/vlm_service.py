import os
import logging
from PIL import Image
from vllm import LLM, SamplingParams
from config import Config

# WSL2 ç¯å¢ƒä¼˜åŒ–
os.environ["VLLM_USE_MODELSCOPE"] = "True"
os.environ["NCCL_P2P_DISABLE"] = "1"
os.environ["NCCL_IB_DISABLE"] = "1"

logger = logging.getLogger(__name__)

class VLMService:
    _instance = None

    def __init__(self):
        self.model_path = Config.VLM_MODEL_PATH
        logger.info(f"ğŸ¨ [vLLM] æ­£åœ¨ A4000 å¯åŠ¨è‡ªé€‚åº”è§†è§‰å¼•æ“: {self.model_path}")

        try:
            # ğŸ’¡ é’ˆå¯¹ 16GB æ˜¾å­˜çš„æœ€ç»ˆå¹³è¡¡æ–¹æ¡ˆ
            self.model = LLM(
                model=self.model_path,
                trust_remote_code=True,
                # ğŸ”¥ è°ƒæ•´ 1ï¼šåˆ©ç”¨ç‡æå‡åˆ° 0.7 (çº¦ 11.2GB)ï¼Œç»™ KV Cache ç•™è¶³ç©ºé—´
                gpu_memory_utilization=0.7,
                # ğŸ”¥ è°ƒæ•´ 2ï¼šä¸Šé™æå‡åˆ° 2048ï¼Œè¶³ä»¥å®¹çº³ç¼©æ”¾åçš„å›¾ç‰‡
                max_model_len=2048,
                limit_mm_per_prompt={"image": 1},
                enforce_eager=True
            )

            self.sampling_params = SamplingParams(
                temperature=0.1,
                top_p=0.9,
                max_tokens=512,
                stop=["<|endoftext|>", "<|im_end|>"]
            )
            logger.info("âœ… vLLM è§†è§‰å¼•æ“å·²æˆåŠŸå…¥é©»å¹¶å®Œæˆè‡ªé€‚åº”é…ç½®ï¼")
        except Exception as e:
            logger.error(f"âŒ vLLM åˆå§‹åŒ–å¤±è´¥: {e}")
            raise e

    @classmethod
    def get_instance(cls):
        if cls._instance is None:
            cls._instance = cls()
        return cls._instance

    def describe_image(self, image_path: str, context_breadcrumb: str = "", is_table: bool = False) -> str:
        """
        å¸¦ä¸Šä¸‹æ–‡å¼•å¯¼çš„è§†è§‰æ¨ç†
        """
        # ğŸ’¡ é’ˆå¯¹ä¸åŒç±»å‹çš„å›¾ï¼Œä½¿ç”¨ä¸åŒçš„å¼•å¯¼è¯­
        if is_table:
            prompt = (
                "è¿™æ˜¯ä¸€å¼ ä»å­¦æœ¯è®ºæ–‡ä¸­æå–çš„è¡¨æ ¼å›¾ç‰‡ã€‚"
                "1. è¯·é¦–å…ˆåœ¨å›¾ç‰‡ä¸­å¯»æ‰¾ç±»ä¼¼ 'Table 1', 'Table 2' çš„æ–‡å­—æ ‡è¯†ï¼Œå¹¶ä»¥æ­¤ä½œä¸ºå¼€å¤´ã€‚"
                "2. è¯·å°†è¡¨æ ¼å†…å®¹å®Œæ•´ã€ç²¾ç¡®åœ°è½¬å½•ä¸º Markdown æ ¼å¼ã€‚"
                "3. ä¸¥ç¦æ¦‚æ‹¬ï¼Œå¿…é¡»ä¿ç•™æ¯ä¸€è¡Œã€æ¯ä¸€åˆ—çš„åŸå§‹æ•°å€¼å’Œå•ä½ã€‚"
                f"4. å‚è€ƒä¸Šä¸‹æ–‡ï¼šè¯¥å›¾å¯èƒ½ä½äºæ–‡æ¡£çš„ {context_breadcrumb} ç« èŠ‚ã€‚"
            )
        else:
            prompt = f"è¿™å¼ å›¾ç‰‡ä½äº '{context_breadcrumb}'ã€‚è¯·è¯¦ç»†è¯†åˆ«å›¾ä¸­çš„æ¶æ„ç»„ä»¶ã€ç®­å¤´æµå‘ã€æ–‡å­—è¯´æ˜ã€‚å¦‚æœæ˜¯æµç¨‹å›¾ï¼Œè¯·åˆ—å‡ºä» A åˆ° B çš„å…·ä½“æ­¥éª¤ã€‚"

        input_prompt = (
            f"<|im_start|>system\nYou are a helpful assistant.<|im_end|>\n"
            f"<|im_start|>user\n<|vision_start|><|image_pad|><|vision_end|>{prompt}<|im_end|>\n"
            f"<|im_start|>assistant\n"
        )

        try:
            # æ‰‹åŠ¨ç¼©æ”¾å›¾ç‰‡é˜²æ­¢ Token æº¢å‡º
            # Qwen2-VL æ¯ä¸ª 28x28 çš„åˆ‡ç‰‡æ˜¯ä¸€ä¸ª Token
            # é™åˆ¶æ€»åƒç´ åœ¨ 250,000 å·¦å³ï¼ˆçº¦ç­‰äº 500x500ï¼‰ï¼Œäº§ç”Ÿçº¦ 400-600 ä¸ª Token
            raw_image = Image.open(image_path).convert("RGB")

            # åŠ¨æ€è®¡ç®—ç¼©æ”¾æ¯”ä¾‹
            max_pixels = 600000
            width, height = raw_image.size
            if width * height > max_pixels:
                scale = (max_pixels / (width * height)) ** 0.5
                new_size = (int(width * scale), int(height * scale))
                image = raw_image.resize(new_size, Image.LANCZOS)
                logger.info(f"ğŸ“ å›¾ç‰‡å·²ä» {width}x{height} ç¼©æ”¾è‡³ {new_size}")
            else:
                image = raw_image

            outputs = self.model.generate(
                {
                    "prompt": input_prompt,
                    "multi_modal_data": {"image": image},
                },
                sampling_params=self.sampling_params
            )
            return outputs[0].outputs[0].text
        except Exception as e:
            logger.error(f"âŒ æ¨ç†å¤±è´¥: {e}")
            return f"[è§†è§‰è§£æå¼‚å¸¸]: {str(e)}"