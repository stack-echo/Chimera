import logging
import io
import pandas as pd
from pathlib import Path
from typing import List, Dict, Any

# Docling æ ¸å¿ƒç»„ä»¶
from docling.document_converter import DocumentConverter, PdfFormatOption
from docling.datamodel.base_models import InputFormat
from docling.datamodel.pipeline_options import PdfPipelineOptions
from docling.datamodel.document import DocumentStream
from docling.chunking import HybridChunker

# ğŸ”¥ æ ¸å¿ƒä¿®æ­£ï¼šä»…å¼•å…¥ Labelï¼Œä¸å†å°è¯•å¼•å…¥ä¸å­˜åœ¨çš„ HeadingItem
from docling_core.types.doc import DocItemLabel

class DoclingParser:
    _converter = None
    _chunker = None

    @classmethod
    def _get_components(cls):
        """å•ä¾‹æ¨¡å¼åˆå§‹åŒ–"""
        if cls._converter is None:
            logging.info("ğŸ¢ [Init] åˆå§‹åŒ– Docling v2 å±‚æ¬¡åŒ–å¼•æ“...")
            pipeline_options = PdfPipelineOptions()
            pipeline_options.do_ocr = False
            pipeline_options.do_table_structure = True

            cls._converter = DocumentConverter(
                format_options={
                    InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)
                }
            )

            # è°ƒä½ max_tokens è§£å†³ä¸Šä¸€æ­¥æåˆ°çš„ (531 > 512) è­¦å‘Š
            cls._chunker = HybridChunker(
                tokenizer="sentence-transformers/all-MiniLM-L6-v2",
                max_tokens=400,
                merge_peers=True,
            )
            logging.info("âœ… [Init] Docling å¼•æ“å°±ç»ª")
        return cls._converter, cls._chunker

    @staticmethod
    def _get_header_path(item, doc) -> List[str]:
        """
        Docling v2 å…¼å®¹é€»è¾‘ï¼šåˆ©ç”¨ label åˆ¤æ–­æ ‡é¢˜å¹¶æº¯æº
        """
        path = []
        try:
            curr = item
            # v2 ä¸­çˆ¶èŠ‚ç‚¹å¼•ç”¨é€šå¸¸åœ¨ item.parent ä¸­
            while curr and hasattr(curr, "parent") and curr.parent is not None:
                # ä½¿ç”¨ doc[index] æˆ– doc.get_item è®¿é—®çˆ¶èŠ‚ç‚¹
                # æ³¨æ„ï¼šåœ¨æŸäº›ç‰ˆæœ¬ä¸­ doc[curr.parent] æ˜¯æ ‡å‡†å†™æ³•
                parent_item = doc[curr.parent]

                # ğŸ”¥ ä½¿ç”¨ label è¿›è¡Œç±»å‹åˆ¤æ–­
                if parent_item.label == DocItemLabel.HEADING:
                    path.insert(0, parent_item.text.strip())
                curr = parent_item
        except Exception as e:
            # æº¯æºå¼‚å¸¸é€šå¸¸æ˜¯å› ä¸ºåˆ°è¾¾äº†æ ¹èŠ‚ç‚¹æˆ–ç»“æ„æ–­è£‚ï¼Œé™é»˜å¤„ç†
            pass
        return path

    @staticmethod
    def _table_to_propositions(table_item, doc) -> str:
        """
        è¡¨æ ¼å‘½é¢˜åŒ–å®ç° (Task 1.3)
        """
        try:
            df = table_item.export_to_dataframe(doc)
            if df is None or df.empty:
                return ""

            propositions = []
            table_title = "æ•°æ®è¡¨"
            if hasattr(table_item, 'caption') and table_item.caption:
                table_title = table_item.caption.text.strip()

            for idx, row in df.iterrows():
                row_header = f"ç¬¬{idx+1}è¡Œ"
                for col in df.columns:
                    val = row[col]
                    if pd.isna(val) or str(val).strip() == "":
                        continue
                    # æ„é€ é™ˆè¿°å¥å¢å¼ºè¯­ä¹‰æœç´¢
                    prop = f"åœ¨ã€Š{table_title}ã€‹ä¸­ï¼Œ{row_header}çš„â€œ{col}â€æ˜¯â€œ{val}â€ã€‚"
                    propositions.append(prop)

            return "\n".join(propositions)
        except Exception as e:
            logging.warning(f"âš ï¸ è¡¨æ ¼å¤„ç†è·³è¿‡: {e}")
            return ""

    @staticmethod
    def parse_and_chunk(file_source, filename="temp.pdf") -> List[Dict[str, Any]]:
        converter, chunker = DoclingParser._get_components()
        logging.info(f"ğŸ“„ [Docling] æ­£åœ¨è§£æ: {filename}")

        try:
            if isinstance(file_source, bytes):
                input_doc = DocumentStream(name=filename, stream=io.BytesIO(file_source))
            else:
                input_doc = Path(file_source)

            conv_result = converter.convert(input_doc)
            doc = conv_result.document # v2 DoclingDocument å¯¹è±¡

            # æ‰§è¡Œåˆ‡åˆ†
            chunk_iter = chunker.chunk(doc)

            final_chunks = []
            for i, chunk in enumerate(chunk_iter):
                header_path = []
                page_num = 1
                processed_content = chunk.text

                # æº¯æºå±‚çº§ä¸è¡¨æ ¼é€»è¾‘
                if chunk.meta.doc_items:
                    first_item = chunk.meta.doc_items[0]

                    # 1. æå–é¢åŒ…å±‘è·¯å¾„
                    header_path = DoclingParser._get_header_path(first_item, doc)

                    # 2. æå–é¡µç 
                    if hasattr(first_item, 'prov') and first_item.prov:
                        page_num = first_item.prov[0].page_no

                    # 3. å¦‚æœæ˜¯è¡¨æ ¼ï¼Œåº”ç”¨å‘½é¢˜åŒ–è½¬æ¢
                    if first_item.label == DocItemLabel.TABLE:
                        table_props = DoclingParser._table_to_propositions(first_item, doc)
                        if table_props:
                            processed_content = table_props

                breadcrumb = " > ".join(header_path)
                # èåˆ Tree-T ç»“æ„ä¸æ­£æ–‡
                enriched_content = f"ã€ä½ç½®: {breadcrumb}ã€‘\n{processed_content}" if breadcrumb else processed_content

                final_chunks.append({
                    "content": enriched_content,
                    "metadata": {
                        "header_path": header_path,
                        "breadcrumb": breadcrumb,
                        "level": len(header_path),
                        "page_number": page_num,
                        "file_name": filename
                    }
                })

            logging.info(f"âœ‚ï¸ [Tree-T] å·²ç”Ÿæˆ {len(final_chunks)} ä¸ªé«˜è´¨é‡åˆ‡ç‰‡")
            return final_chunks

        except Exception as e:
            logging.error(f"âŒ [Docling] è§£æå´©æºƒ: {e}", exc_info=True)
            return []